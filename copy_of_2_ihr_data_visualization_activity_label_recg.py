# -*- coding: utf-8 -*-
"""Copy of 2. IHR - Data Visualization -  Activity Label Recg.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1quBiHb_fBDACohJ-6gMh3YPp9mK61G4C

Hyperparameter	Value


frame_size (sequence length)	100


feature_cols	6 features (acceleration X,Y,Z + angular X,Y,Z)


d_model (embedding size)	64


n_head (attention heads)	4


n_layers (encoder layers)	2


hidden (feedforward size)	128


dropout	0.3


n_classes	4 (HES, HER, TOF, FOF)


batch_size	32


learning_rate	5e-5


optimizer	AdamW (weight_decay=1e-4)


epochs	50


loss_fn	Focal Loss (Î³=2, Î±=class weights)


cross-validation folds	5
"""

import os, math, copy, torch
import numpy as np, pandas as pd
import matplotlib.pyplot as plt, seaborn as sns
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.utils.class_weight import compute_class_weight
from imblearn.over_sampling import RandomOverSampler
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn, torch.nn.functional as F
from torch.optim import AdamW
from torch.cuda.amp import GradScaler, autocast
from collections import Counter

from google.colab import drive
drive.mount('/content/drive')

# ========================== CONFIG ==============================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
frame_size = 100
feature_cols = [
    'left acceleration X[g]',
    'left acceleration Y[g]',
    'left acceleration Z[g]',
    'left angular X[dps]',
    'left angular Y[dps]',
    'left angular Z[dps]',
]
label_map = {'HES':0,'HER':1,'TOF':2,'FOF':3}

# ========================== DATA AUGMENTATION ==============================
def augment_segment(segment):
    # Add Gaussian noise
    noise = np.random.normal(0, 0.01, segment.shape)
    scale = np.random.uniform(0.9, 1.1)
    shift = np.random.randint(-5, 5)

    segment_aug = segment.copy()
    segment_aug = (segment_aug + noise) * scale
    segment_aug = np.roll(segment_aug, shift, axis=0)  # time shift
    return segment_aug

def balance_with_augmentation(frames, labels, target_count=None):
    frames_balanced = []
    labels_balanced = []

    counts = Counter(labels)
    if target_count is None:
        target_count = max(counts.values())  # match majority class

    for class_idx in range(len(label_map)):
        class_frames = frames[labels == class_idx]
        n_samples = len(class_frames)

        frames_balanced.extend(class_frames)
        labels_balanced.extend([class_idx] * n_samples)

        if n_samples < target_count:
            n_to_add = target_count - n_samples
            for _ in range(n_to_add):
                frame = class_frames[np.random.randint(0, n_samples)]
                frame_reshaped = frame.reshape(frame_size, len(feature_cols))
                aug_frame = augment_segment(frame_reshaped).flatten()
                frames_balanced.append(aug_frame)
                labels_balanced.append(class_idx)

    return np.array(frames_balanced), np.array(labels_balanced)

# ========================== DATASET ==============================
class IMUDataset(Dataset):
    def __init__(self, X, Y):
        self.X = torch.tensor(X, dtype=torch.float32).reshape(-1, frame_size, len(feature_cols))
        self.Y = torch.tensor(Y, dtype=torch.long)
    def __len__(self): return len(self.Y)
    def __getitem__(self, idx): return self.X[idx], self.Y[idx]

# ====================== DATA PROCESSING ===========================
def process_file(filepath):
    df = pd.read_excel(filepath)
    df = df.dropna(subset=feature_cols+['Event- Label level 2- Left Foot'])
    df['label'] = df['Event- Label level 2- Left Foot'].map(label_map)
    df = df.dropna(subset=['label'])
    df['label'] = df['label'].astype(int)

    frames, labels = [], []
    for i in range(0, len(df)-frame_size+1, frame_size):
        segment = df.iloc[i:i+frame_size]
        label = np.bincount(segment['label']).argmax()
        frames.append(segment[feature_cols].values.flatten())
        labels.append(label)
    return frames, labels

def load_all_group_data(root_dir, groups):
    all_frames, all_labels = [], []
    print("\nLoading data...")
    for group in groups:
        group_path = os.path.join(root_dir, group)
        for file in os.listdir(group_path):
            if file.endswith('.xlsx'):
                try:
                    frames, labels = process_file(os.path.join(group_path, file))
                    all_frames += list(frames)
                    all_labels += list(labels)
                except KeyError as e:
                    print(f"File skipped due to missing columns: {file}")
                    print(f"Missing columns error: {e}")
                    continue

    all_frames, all_labels = np.array(all_frames), np.array(all_labels)

    print("Loaded:", all_frames.shape, "Labels:", Counter(all_labels))

    # Plot class distribution
    counts = Counter(all_labels)
    plt.figure(figsize=(6,4))
    plt.bar([list(label_map.keys())[i] for i in counts.keys()], counts.values(), color='skyblue')
    plt.title('Class Distribution After Loading Data')
    plt.xlabel('Classes')
    plt.ylabel('Number of Samples')
    plt.show()

    return all_frames, all_labels

# ==================== CROSS VALIDATION =============================
def cross_validate(frames, labels, root_dir, k=5, epochs=50, batch_size=32, lr=1e-4):
    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)
    results = []
    scaler = StandardScaler()
    frames = scaler.fit_transform(frames)

    # compute class weights for loss
    weights = compute_class_weight('balanced', classes=np.arange(4), y=labels)
    alpha = torch.tensor(weights, dtype=torch.float32).to(device)

    for fold, (tr, va) in enumerate(skf.split(frames, labels), 1):
        print(f"\n===== Fold {fold}/{k} =====")
        Xtr, Ytr = frames[tr], labels[tr]
        Xva, Yva = frames[va], labels[va]

        # ðŸ”¥ BALANCE TRAIN DATA WITH AUGMENTATION
        Xtr, Ytr = balance_with_augmentation(Xtr, Ytr)

        print("After augmentation:", Counter(Ytr))

        # plot new distribution
        counts = Counter(Ytr)
        plt.figure(figsize=(6,4))
        plt.bar([list(label_map.keys())[i] for i in counts.keys()], counts.values(), color='orange')
        plt.title(f'Class Distribution Fold {fold} (Train After Augmentation)')
        plt.xlabel('Classes')
        plt.ylabel('Number of Samples')
        plt.show()

        train_loader = DataLoader(IMUDataset(Xtr, Ytr), batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(IMUDataset(Xva, Yva), batch_size=batch_size)

        model = TransformerModel().to(device)
        opt = AdamW(model.parameters(), lr=lr, weight_decay=1e-4)
        loss_fn = FocalLoss(alpha)
        amp_scaler = GradScaler()

        best_acc = 0
        best_wts = copy.deepcopy(model.state_dict())
        history = {"train_loss": [], "val_loss": [], "train_acc": [], "val_acc": []}

        for ep in range(epochs):
            tr_loss, tr_acc = train_one_epoch(model, train_loader, opt, amp_scaler, loss_fn)
            va_loss, va_acc, preds, labs = validate(model, val_loader, loss_fn)

            history["train_loss"].append(tr_loss)
            history["val_loss"].append(va_loss)
            history["train_acc"].append(tr_acc)
            history["val_acc"].append(va_acc)

            print(f"Epoch {ep+1}: Train Acc={tr_acc:.3f} | Val Acc={va_acc:.3f}")

            if va_acc > best_acc:
                best_acc = va_acc
                best_wts = copy.deepcopy(model.state_dict())

        # plots and metrics
        epochs_range = range(1, len(history["train_loss"])+1)
        plt.figure(figsize=(12,5))
        plt.subplot(1,2,1)
        plt.plot(epochs_range, history["train_loss"], label="Train Loss")
        plt.plot(epochs_range, history["val_loss"], label="Val Loss")
        plt.xlabel("Epochs"); plt.ylabel("Loss"); plt.legend(); plt.title(f"Fold {fold} Loss")

        plt.subplot(1,2,2)
        plt.plot(epochs_range, history["train_acc"], label="Train Acc")
        plt.plot(epochs_range, history["val_acc"], label="Val Acc")
        plt.xlabel("Epochs"); plt.ylabel("Accuracy"); plt.legend(); plt.title(f"Fold {fold} Accuracy")
        plt.show()

        model.load_state_dict(best_wts)
        acc = accuracy_score(labs, preds)
        prec = precision_score(labs, preds, average='weighted')
        rec = recall_score(labs, preds, average='weighted')
        f1 = f1_score(labs, preds, average='weighted')

        print(f"Fold {fold} Final: Acc={acc:.3f} Prec={prec:.3f} Rec={rec:.3f} F1={f1:.3f}")

        torch.save(model.state_dict(), f"{root_dir}/transformer_fold{fold}.pth")
        cm = confusion_matrix(labs, preds)
        sns.heatmap(cm, annot=True, fmt='d', xticklabels=label_map.keys(), yticklabels=label_map.keys())
        plt.title(f"Confusion Matrix Fold {fold}")
        plt.show()

        results.append({'fold': fold, 'acc': acc, 'prec': prec, 'rec': rec, 'f1': f1})

    df = pd.DataFrame(results)
    df.to_csv(f"{root_dir}/cv_metrics.csv", index=False)
    print("\nAverage:", df[['acc', 'prec', 'rec', 'f1']].mean())
    return df

# ===================== TRANSFORMER MODEL =========================
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        pos = torch.arange(0,max_len).unsqueeze(1)
        div = torch.exp(torch.arange(0,d_model,2)*(-math.log(10000.0)/d_model))
        pe[:,0::2], pe[:,1::2] = torch.sin(pos*div), torch.cos(pos*div)
        self.register_buffer('pe', pe.unsqueeze(0))
    def forward(self,x):
        return x+self.pe[:,:x.size(1)]

class MultiHeadAttention(nn.Module):
    def __init__(self,d_model,n_head):
        super().__init__(); self.n_head=n_head; self.d_head=d_model//n_head
        self.qkv=nn.Linear(d_model,3*d_model); self.o=nn.Linear(d_model,d_model)
    def forward(self,x):
        B,T,C=x.shape
        qkv=self.qkv(x).reshape(B,T,3,self.n_head,self.d_head).permute(2,0,3,1,4)
        q,k,v=qkv[0],qkv[1],qkv[2]
        attn=torch.matmul(q,k.transpose(-2,-1))/math.sqrt(self.d_head)
        attn=attn.softmax(-1)
        out=torch.matmul(attn,v).transpose(1,2).reshape(B,T,C)
        return self.o(out)

class FeedForward(nn.Module):
    def __init__(self,d_model,hidden,drop=0.3):
        super().__init__()
        self.net=nn.Sequential(nn.Linear(d_model,hidden),nn.ReLU(),nn.Dropout(drop),nn.Linear(hidden,d_model))
    def forward(self,x): return self.net(x)

class EncoderLayer(nn.Module):
    def __init__(self,d_model,n_head,hidden,drop=0.3):
        super().__init__()
        self.attn=MultiHeadAttention(d_model,n_head)
        self.norm1, self.norm2 = nn.LayerNorm(d_model), nn.LayerNorm(d_model)
        self.ff=FeedForward(d_model,hidden,drop)
    def forward(self,x):
        x=self.norm1(x+self.attn(x))
        return self.norm2(x+self.ff(x))

class TransformerModel(nn.Module):
    def __init__(self,d_model=64,n_head=4,n_layers=2,hidden=128,n_classes=4,seq_len=100):
        super().__init__()
        self.proj=nn.Sequential(nn.Linear(6,d_model),nn.BatchNorm1d(seq_len))
        self.pos=PositionalEncoding(d_model)
        self.layers=nn.ModuleList([EncoderLayer(d_model,n_head,hidden) for _ in range(n_layers)])
        self.head=nn.Sequential(nn.Flatten(),nn.Linear(d_model*seq_len,256),nn.ReLU(),nn.Dropout(0.3),nn.Linear(256,n_classes))
    def forward(self,x):
        x=self.proj(x); x=self.pos(x)
        for layer in self.layers: x=layer(x)
        return self.head(x)

# ==================== LOSS FUNCTION ===============================
class FocalLoss(nn.Module):
    def __init__(self,alpha=None,gamma=2):
        super().__init__(); self.alpha=alpha; self.gamma=gamma
        self.ce=nn.CrossEntropyLoss(weight=alpha)
    def forward(self,inputs,targets):
        ce_loss=self.ce(inputs,targets)
        pt=torch.exp(-ce_loss)
        return ((1-pt)**self.gamma*ce_loss).mean()

# ==================== TRAINING UTILS ==============================
def train_one_epoch(model,loader,opt,scaler,loss_fn):
    model.train(); total_loss=0; correct=0; total=0
    for x,y in loader:
        x,y=x.to(device),y.to(device); opt.zero_grad()
        with autocast():
            out=model(x); loss=loss_fn(out,y)
        scaler.scale(loss).backward(); scaler.step(opt); scaler.update()
        total_loss+=loss.item()*x.size(0)
        correct+=(out.argmax(1)==y).sum().item(); total+=y.size(0)
    return total_loss/total, correct/total

def validate(model,loader,loss_fn):
    model.eval(); total_loss=0; correct=0; total=0; preds=[]; labels=[]
    with torch.no_grad():
        for x,y in loader:
            x,y=x.to(device),y.to(device)
            out=model(x); loss=loss_fn(out,y)
            total_loss+=loss.item()*x.size(0)
            correct+=(out.argmax(1)==y).sum().item(); total+=y.size(0)
            preds.extend(out.argmax(1).cpu().numpy()); labels.extend(y.cpu().numpy())
    return total_loss/total, correct/total, preds, labels

# ==================== CROSS VALIDATION =============================
def cross_validate(frames,labels,root_dir,k=5,epochs=50,batch_size=32,lr=1e-4):
    skf=StratifiedKFold(n_splits=k,shuffle=True,random_state=42)
    results=[]; scaler=StandardScaler(); frames=scaler.fit_transform(frames)
    weights=compute_class_weight('balanced',classes=np.arange(4),y=labels)
    alpha=torch.tensor(weights,dtype=torch.float32).to(device)

    for fold,(tr,va) in enumerate(skf.split(frames,labels),1):
        print(f"\n===== Fold {fold}/{k} =====")
        Xtr,Ytr=frames[tr],labels[tr]; Xva,Yva=frames[va],labels[va]
        ros=RandomOverSampler(sampling_strategy='auto'); Xtr,Ytr=ros.fit_resample(Xtr,Ytr)
        print("After oversampling:",Counter(Ytr))

        counts = Counter(Ytr)
        plt.figure(figsize=(6,4))
        plt.bar([list(label_map.keys())[i] for i in counts.keys()], counts.values(), color='orange')
        plt.title(f'Class Distribution Fold {fold} (Train After Oversampling)')
        plt.xlabel('Classes')
        plt.ylabel('Number of Samples')
        plt.show()

        train_loader=DataLoader(IMUDataset(Xtr,Ytr),batch_size=batch_size,shuffle=True)
        val_loader=DataLoader(IMUDataset(Xva,Yva),batch_size=batch_size)

        model=TransformerModel().to(device)
        opt=AdamW(model.parameters(),lr=lr,weight_decay=1e-4)
        loss_fn=FocalLoss(alpha)
        amp_scaler=GradScaler()

        best_acc=0
        best_wts = copy.deepcopy(model.state_dict())
        history = {"train_loss": [], "val_loss": [], "train_acc": [], "val_acc": []}

        for ep in range(epochs):
            tr_loss,tr_acc=train_one_epoch(model,train_loader,opt,amp_scaler,loss_fn)
            va_loss,va_acc,preds,labs=validate(model,val_loader,loss_fn)

            history["train_loss"].append(tr_loss)
            history["val_loss"].append(va_loss)
            history["train_acc"].append(tr_acc)
            history["val_acc"].append(va_acc)

            print(f"Epoch {ep+1}: Train Acc={tr_acc:.3f} | Val Acc={va_acc:.3f}")

            if va_acc > best_acc:
                best_acc = va_acc
                best_wts = copy.deepcopy(model.state_dict())

        # Plot loss and accuracy curves for this fold
        epochs_range = range(1, len(history["train_loss"])+1)
        plt.figure(figsize=(12,5))
        plt.subplot(1,2,1)
        plt.plot(epochs_range, history["train_loss"], label="Train Loss")
        plt.plot(epochs_range, history["val_loss"], label="Val Loss")
        plt.xlabel("Epochs"); plt.ylabel("Loss"); plt.legend(); plt.title(f"Fold {fold} Loss")

        plt.subplot(1,2,2)
        plt.plot(epochs_range, history["train_acc"], label="Train Acc")
        plt.plot(epochs_range, history["val_acc"], label="Val Acc")
        plt.xlabel("Epochs"); plt.ylabel("Accuracy"); plt.legend(); plt.title(f"Fold {fold} Accuracy")
        plt.show()

        model.load_state_dict(best_wts)
        acc,prec,rec,f1=(accuracy_score(labs,preds),precision_score(labs,preds,average='weighted'),
                         recall_score(labs,preds,average='weighted'),f1_score(labs,preds,average='weighted'))
        print(f"Fold {fold} Final: Acc={acc:.3f} Prec={prec:.3f} Rec={rec:.3f} F1={f1:.3f}")

        torch.save(model.state_dict(),f"{root_dir}/transformer_fold{fold}.pth")
        cm=confusion_matrix(labs,preds)
        sns.heatmap(cm,annot=True,fmt='d',xticklabels=label_map.keys(),yticklabels=label_map.keys())
        plt.title(f"Confusion Matrix Fold {fold}"); plt.show()

        results.append({'fold':fold,'acc':acc,'prec':prec,'rec':rec,'f1':f1})

    df=pd.DataFrame(results); df.to_csv(f"{root_dir}/cv_metrics.csv",index=False)
    print("\nAverage:",df[['acc','prec','rec','f1']].mean())
    return df

# ==================== USAGE =============================
root_dir = '/content/drive/MyDrive/IMU/Annotated_Data'
groups = ['PD','EL']
frames,labels=load_all_group_data(root_dir,groups)
cross_validate(frames,labels,'/content/drive/MyDrive/IMU/kFold_Results_ActivityRecog_new',k=5,epochs=50,batch_size=32,lr=5e-5)